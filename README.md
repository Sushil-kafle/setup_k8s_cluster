# ğŸš€ Kubernetes Cluster Setup on Rocky Linux 9 with Vagrant, VMware, CRI-O, and Calico

This project automates a local Kubernetes cluster setup using **Vagrant** and **VMware Workstation**. It spins up one control plane (master) node and two worker nodes running **Rocky Linux 9**, with **CRI-O** as the container runtime and **Calico** for networking. ğŸŒ

---

## âœ¨ Features

- **Vagrant + VMware Workstation**: Reproducible environments with infrastructure as code. ğŸ› ï¸
- **Rocky Linux 9**: Stable, RHEL-compatible OS for Kubernetes nodes. ğŸª¨
- **CRI-O**: Lightweight, Kubernetes-optimized container runtime. âš¡
- **Calico**: Powerful CNI plugin for pod networking. ğŸ¾
- **Scalable**: Add or remove worker nodes with ease. ğŸ“ˆ

---

## ğŸ“‹ Requirements

- Vagrant (tested with 2.4.x) ğŸ–¥ï¸

- VMware Workstation Pro (tested with 17.x) ğŸ’»

- Vagrant VMware Plugin:

  ```bash
  vagrant plugin install vagrant-vmware-desktop
  ```

- Minimum 12GB free RAM and 8 CPU cores for VMs ğŸ§ 

- Internet connection for package downloads ğŸŒ

---

## ğŸ“‚ Directory Structure

```
â”œâ”€â”€ Vagrantfile
â”œâ”€â”€ script.sh
â”œâ”€â”€ post_commands.txt
â””â”€â”€ README.md
```

- `Vagrantfile`: Defines 1 master + 2 worker nodes and provisions them. ğŸ“œ
- `script.sh`: Installs CRI-O, Kubernetes tools (kubelet, kubeadm, kubectl), and configures nodes. ğŸ› ï¸
- `post_commands.txt`: Initializes the cluster and deploys Calico. ğŸš€

---

## ğŸ› ï¸ Usage

### 1. Clone the Repository

```bash
git clone <repository-url>
cd <repository-directory>
```

### 2. Spin Up the Cluster

Launch the cluster with:

```bash
vagrant up
```

This creates:

- **Master Node**: `master` (4 CPUs, 8GB RAM, 20GB disk) ğŸ§‘â€ğŸ’¼
- **Worker Nodes**: `worker1`, `worker2` (2 CPUs, 2GB RAM, 20GB disk each) ğŸ‘·

Each node runs **Rocky Linux 9** and is provisioned with CRI-O, kubelet, kubeadm, and kubectl.

---

## âš™ï¸ Post-Setup (On Master Node)

SSH into the master node:

```bash
vagrant ssh master
```

Run the commands in `post_commands.txt` to initialize the cluster and deploy Calico:

```bash
sudo su
cat /vagrant/post_commands.txt | bash
```

This will:

- Initialize the control plane with `kubeadm init`. ğŸ
- Configure `kubectl` for the `vagrant` user. ğŸ”§
- Deploy Calico for pod networking. ğŸ¾

### ğŸŒ Use `kubectl` from Your Local Machine

To manage the cluster locally with `kubectl`:

1. Copy `admin.conf` from the master node:

```bash
vagrant ssh master -c "sudo cat /etc/kubernetes/admin.conf" > ~/.kube/config
```

2. Verify local access:

```bash
kubectl get nodes
```

---

## ğŸ¤ Join Worker Nodes

1. On the master node, generate a join command:

```bash
kubeadm token create --print-join-command
```

2. Copy the output (e.g., `kubeadm join 192.168.56.10:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>`).

3. SSH into each worker node:

```bash
vagrant ssh worker1
vagrant ssh worker2
```

4. Run the join command as root:

```bash
sudo su
<join-command>
```

---

## âœ… Verify Cluster

On the master node or locally (after copying `admin.conf`), check the cluster status:

```bash
kubectl get nodes
kubectl get pods -A
```

Expected output:

- All nodes (`master`, `worker1`, `worker2`) in `Ready` state. âœ…
- Calico and Kubernetes pods running in `kube-system` and `calico-system` namespaces. ğŸ¾

---

## ğŸ“ˆ Add More Workers

To add a new worker node:

1. Edit the `Vagrantfile` and add a new entry to the `WORKER_NODES` map:

```ruby
WORKER_NODES = {
  "worker1" => { :ip => "192.168.56.11", :cpus => 2, :mem => 2048 },
  "worker2" => { :ip => "192.168.56.12", :cpus => 2, :mem => 2048 },
  "worker3" => { :ip => "192.168.56.13", :cpus => 2, :mem => 2048 }
}
```

2. Provision the new node:

```bash
vagrant up worker3
```

3. SSH into the new node and run the `kubeadm join` command from the master. ğŸ¤

---

## ğŸ—‘ï¸ Clean Up

To tear down the cluster and remove all VMs:

```bash
vagrant destroy -f
```

This deletes all nodes and resources. ğŸ§¹

---
